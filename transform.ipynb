{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will be used to contain data processing components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (1.34.74)\n",
      "Requirement already satisfied: pyspark in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: delta-spark in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.74 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from boto3) (1.34.74)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from delta-spark) (7.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from botocore<1.35.0,>=1.34.74->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from botocore<1.35.0,>=1.34.74->boto3) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.74->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 pyspark delta-spark python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following to set the environment variables in the notebook if you don't set manually access key, secret key and endpoint in minio\n",
    "os.environ['OBJ_STORAGE_ACCESS_KEY'] = 'minioadmin'\n",
    "os.environ['OBJ_STORAGE_SECRET_KEY'] = 'minioadmin'\n",
    "os.environ['OBJ_STORAGE_ENDPOINT'] = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 storage\n",
    "obj_storage_access_key = os.getenv('OBJ_STORAGE_ACCESS_KEY')\n",
    "obj_storage_secret_key = os.getenv('OBJ_STORAGE_SECRET_KEY')\n",
    "obj_storage_endpoint = os.getenv('OBJ_STORAGE_ENDPOINT', 'http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = \"s3a://data/data-raw/data.json\"\n",
    "path_2 = \"s3a://data/data-raw/data2.json\"\n",
    "path_3 = \"s3a://data/data-raw/data3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 20:54:09 WARN Utils: Your hostname, Hoang-Nhans-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.3 instead (on interface en0)\n",
      "24/03/30 20:54:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/hoang-nhannguyen/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/hoang-nhannguyen/.ivy2/jars\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4bc1b091-9e9b-4b7e-85f8-ddc67ed5b94c;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.690 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 881ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.690 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.690] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   96  |   0   |   0   |   1   ||   95  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4bc1b091-9e9b-4b7e-85f8-ddc67ed5b94c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 95 already retrieved (0kB/11ms)\n",
      "24/03/30 20:54:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# You need to more configuration if you want to use minio as object storage \n",
    "# (hint: maybe you can using .config() method to set the configuration if you want using spark to read/write data from/to minio)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.amazonaws:aws-java-sdk-bundle:1.12.690,org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", obj_storage_endpoint) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", obj_storage_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", obj_storage_secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", 'false') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", 15000) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", 15000) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 20:54:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "st_df = spark.read.format(\"json\").option(\"multiLine\", True).option(\"header\",True).option(\"inferschema\",True).load(path_1)\n",
    "nd_df = spark.read.format(\"json\").option(\"multiLine\", True).option(\"header\",True).option(\"inferschema\",True).load(path_2)\n",
    "rd_df = spark.read.format(\"json\").option(\"multiLine\", True).option(\"header\",True).option(\"inferschema\",True).load(path_3)\n",
    "schema = st_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 20:54:15 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/03/30 20:54:15 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/03/30 20:54:15 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/03/30 20:54:15 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    }
   ],
   "source": [
    "json_path = \"s3a://data/data-result/result.json\"\n",
    "merged_df = st_df.union(nd_df).union(rd_df)\n",
    "merged_df.write.format(\"json\").mode(\"overwrite\").save(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+-------------------+----+--------------------+-----+\n",
      "|             batters|             filling|  id|               name| ppu|             topping| type|\n",
      "+--------------------+--------------------+----+-------------------+----+--------------------+-----+\n",
      "|{[{1001, Regular}...|                NULL|0004|              Jelly|0.65|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0005|     Custard-Filled|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|[{6001, None}, {6...|0006|     Cinnamon Twist|0.85|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0007|    Vanilla Frosted|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0008| Strawberry Frosted|0.85|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0009|     Chocolate Cake|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0010|     Blueberry Cake|0.55|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0011|  Devil's Food Cake|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0017|      Old Fashioned|0.65|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0018|            Regular|0.55|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|[{6001, None}, {6...|0019|          Blueberry|0.75|[{5002, Glazed}, ...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0004|              Jelly|0.65|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0020|       Devil's Food|0.75|[{5002, Glazed}, ...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0021|      Vanilla Cream|0.75|[{5002, Glazed}, ...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0012|        Jelly Donut|0.65|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0013|       Raised Donut|0.55|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0014|Old Fashioned Donut|0.75|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0015|            Cruller|0.85|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|[{6001, None}, {6...|0016|       Filled Donut|0.95|[{5001, None}, {5...|donut|\n",
      "+--------------------+--------------------+----+-------------------+----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+-------------------+----+--------------------+-----+\n",
      "|             batters|             filling|  id|               name| ppu|             topping| type|\n",
      "+--------------------+--------------------+----+-------------------+----+--------------------+-----+\n",
      "|{[{1001, Regular}...|                NULL|0004|              Jelly|0.65|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0011|  Devil's Food Cake|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0010|     Blueberry Cake|0.55|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0007|    Vanilla Frosted|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|[{6001, None}, {6...|0006|     Cinnamon Twist|0.85|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0009|     Chocolate Cake|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0005|     Custard-Filled|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0008| Strawberry Frosted|0.85|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0018|            Regular|0.55|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0021|      Vanilla Cream|0.75|[{5002, Glazed}, ...|donut|\n",
      "| {[{1001, Regular}]}|[{6001, None}, {6...|0019|          Blueberry|0.75|[{5002, Glazed}, ...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0017|      Old Fashioned|0.65|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0020|       Devil's Food|0.75|[{5002, Glazed}, ...|donut|\n",
      "|{[{1001, Regular}...|[{6001, None}, {6...|0016|       Filled Donut|0.95|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0013|       Raised Donut|0.55|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0014|Old Fashioned Donut|0.75|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|                NULL|0012|        Jelly Donut|0.65|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|                NULL|0015|            Cruller|0.85|[{5001, None}, {5...|donut|\n",
      "+--------------------+--------------------+----+-------------------+----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "nested_df = df.select(\n",
    "    col(\"id\"),\n",
    "    col(\"type\"),\n",
    "    col(\"name\"),\n",
    "    col(\"ppu\"),\n",
    "    col(\"batters\").cast(\"string\").alias(\"batters\"),\n",
    "    col(\"topping\").cast(\"string\").alias(\"topping\"),\n",
    "    col(\"filling\").cast(\"string\").alias(\"filling\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+----+--------------------+--------------------+--------------------+\n",
      "|  id| type|               name| ppu|             batters|             topping|             filling|\n",
      "+----+-----+-------------------+----+--------------------+--------------------+--------------------+\n",
      "|0004|donut|              Jelly|0.65|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0011|donut|  Devil's Food Cake|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0010|donut|     Blueberry Cake|0.55|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0007|donut|    Vanilla Frosted|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0006|donut|     Cinnamon Twist|0.85|{[{1001, Regular}...|[{5001, None}, {5...|[{6001, None}, {6...|\n",
      "|0009|donut|     Chocolate Cake|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0005|donut|     Custard-Filled|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0008|donut| Strawberry Frosted|0.85|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0018|donut|            Regular|0.55| {[{1001, Regular}]}|[{5001, None}, {5...|                NULL|\n",
      "|0021|donut|      Vanilla Cream|0.75|{[{1001, Regular}...|[{5002, Glazed}, ...|                NULL|\n",
      "|0019|donut|          Blueberry|0.75| {[{1001, Regular}]}|[{5002, Glazed}, ...|[{6001, None}, {6...|\n",
      "|0017|donut|      Old Fashioned|0.65|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0020|donut|       Devil's Food|0.75| {[{1001, Regular}]}|[{5002, Glazed}, ...|                NULL|\n",
      "|0016|donut|       Filled Donut|0.95|{[{1001, Regular}...|[{5001, None}, {5...|[{6001, None}, {6...|\n",
      "|0013|donut|       Raised Donut|0.55| {[{1001, Regular}]}|[{5001, None}, {5...|                NULL|\n",
      "|0014|donut|Old Fashioned Donut|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0012|donut|        Jelly Donut|0.65|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0015|donut|            Cruller|0.85| {[{1001, Regular}]}|[{5001, None}, {5...|                NULL|\n",
      "+----+-----+-------------------+----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 20:54:18 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/03/30 20:54:18 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"s3a://data/data-result/result.csv\"\n",
    "\n",
    "nested_df.write.mode(\"overwrite\").csv(csv_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+----+--------------------+--------------------+--------------------+\n",
      "|  id| type|               name| ppu|             batters|             topping|             filling|\n",
      "+----+-----+-------------------+----+--------------------+--------------------+--------------------+\n",
      "|0004|donut|              Jelly|0.65|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0011|donut|  Devil's Food Cake|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0010|donut|     Blueberry Cake|0.55|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0007|donut|    Vanilla Frosted|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0006|donut|     Cinnamon Twist|0.85|{[{1001, Regular}...|[{5001, None}, {5...|[{6001, None}, {6...|\n",
      "|0009|donut|     Chocolate Cake|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0005|donut|     Custard-Filled|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0008|donut| Strawberry Frosted|0.85|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0018|donut|            Regular|0.55| {[{1001, Regular}]}|[{5001, None}, {5...|                NULL|\n",
      "|0021|donut|      Vanilla Cream|0.75|{[{1001, Regular}...|[{5002, Glazed}, ...|                NULL|\n",
      "|0019|donut|          Blueberry|0.75| {[{1001, Regular}]}|[{5002, Glazed}, ...|[{6001, None}, {6...|\n",
      "|0017|donut|      Old Fashioned|0.65|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0020|donut|       Devil's Food|0.75| {[{1001, Regular}]}|[{5002, Glazed}, ...|                NULL|\n",
      "|0016|donut|       Filled Donut|0.95|{[{1001, Regular}...|[{5001, None}, {5...|[{6001, None}, {6...|\n",
      "|0013|donut|       Raised Donut|0.55| {[{1001, Regular}]}|[{5001, None}, {5...|                NULL|\n",
      "|0014|donut|Old Fashioned Donut|0.75|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0012|donut|        Jelly Donut|0.65|{[{1001, Regular}...|[{5001, None}, {5...|                NULL|\n",
      "|0015|donut|            Cruller|0.85| {[{1001, Regular}]}|[{5001, None}, {5...|                NULL|\n",
      "+----+-----+-------------------+----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 20:54:23 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/03/30 21:12:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 420943 ms exceeds timeout 120000 ms\n",
      "24/03/30 21:12:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "24/03/30 21:12:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.3:52894\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/03/30 21:12:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.3:52894\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
